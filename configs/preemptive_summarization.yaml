# =============================================================================
# Context Gateway - Production Configuration
# =============================================================================
# Preemptive Summarization: Background summarization for instant compaction.
# When context reaches 85%, a summary is generated in the background.
# When user calls /compact, the summary is already ready - zero wait time.
#
# This is the only enabled feature in the current release.
# =============================================================================

metadata:
  name: "Preemptive Summarization"
  description: "Background summarization for instant compaction"
  strategy: "passthrough"

server:
  port: ${GATEWAY_PORT:-18080}
  read_timeout: 30s
  # Long timeout for LLM responses with extended thinking (can take 5+ min)
  write_timeout: 1000s

urls:
  gateway: "http://localhost:${GATEWAY_PORT:-18080}"

# =============================================================================
# PREEMPTIVE SUMMARIZATION
# =============================================================================

preemptive:
  enabled: true
  
  # Trigger at 85% context usage - gives time for summary to complete
  trigger_threshold: 85.0
  
  # Add response headers for debugging (X-Context-Usage, X-Summary-Ready, etc.)
  add_response_headers: true
  
  # Logging
  log_dir: "${SESSION_DIR:-logs}"
  compaction_log_path: "${SESSION_COMPACTION_LOG:-logs/compaction.jsonl}"
  
  summarizer:
    # Provider is auto-detected from endpoint URL:
    #   - api.anthropic.com → Anthropic
    #   - generativelanguage.googleapis.com → Gemini
    #   - anything else → OpenAI-compatible
    #
    # Anthropic (default):
    model: "claude-haiku-4-5"
    api_key: "${ANTHROPIC_API_KEY}"
    endpoint: "https://api.anthropic.com/v1/messages"
    #
    # Gemini example:
    #   model: "gemini-2.0-flash"
    #   api_key: "${GEMINI_API_KEY}"
    #   endpoint: "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
    #
    # OpenAI example:
    #   model: "gpt-4o-mini"
    #   api_key: "${OPENAI_API_KEY}"
    #   endpoint: "https://api.openai.com/v1/chat/completions"
    
    max_tokens: 4096
    timeout: 60s
    token_estimate_ratio: 4
  
  session:
    summary_ttl: 3h
    hash_message_count: 3
  
  # Detectors use built-in defaults - no need to override
  detectors:
    claude_code:
      enabled: true
    codex:
      enabled: true
    generic:
      enabled: true
      header_name: "X-Request-Compaction"
      header_value: "true"

# =============================================================================
# COMPRESSION PIPES - Disabled (preemptive only)
# =============================================================================

pipes:
  history:
    enabled: false
  tool_output:
    enabled: false
  tool_discovery:
    enabled: false

# =============================================================================
# STORE
# =============================================================================

store:
  type: "memory"
  ttl: 1h

# =============================================================================
# MONITORING
# =============================================================================

monitoring:
  log_level: "info"
  log_format: "console"
  log_output: "stdout"
  telemetry_enabled: true
  telemetry_path: "${SESSION_TELEMETRY_LOG:-logs/telemetry.jsonl}"
  compression_log_path: "${SESSION_COMPRESSION_LOG:-logs/compression.jsonl}"
